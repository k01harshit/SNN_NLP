# -*- coding: utf-8 -*-
"""nlp_snn_funs106

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12OCTtoANpcWdHLESpVYGkowxh7_dzvOZ
"""

# ==============================================================================
# NLP-SNN BENCHMARKING FUNCTIONS (v7.0 - Advanced Encoders & 1-SADP)
# ==============================================================================

import numpy as np
import tensorflow as tf
import tensorflow_hub as hub
import os
import time
import datetime
from tensorflow.keras import layers, models
import tensorflow_datasets as tfds

# 1. CRITICAL: Import tensorflow_text for BERT/USE models
try:
    import tensorflow_text as text
    print(f"[{datetime.datetime.now().strftime('%H:%M:%S')}] [System] tensorflow_text imported successfully.")
except ImportError:
    print(f"[{datetime.datetime.now().strftime('%H:%M:%S')}] [System] WARNING: tensorflow_text not found. TFHub models will crash.")

# Set seeds
np.random.seed(42)
tf.random.set_seed(42)

def log(msg):
    """Helper for consistent detailed logging with timestamps."""
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] {msg}")

# ==============================================================================
# 2. ROBUST DATA LOADING
# ==============================================================================

def load_nlp_dataset(dataset_name, max_samples=None):
    """
    Loads dataset and converts to Numpy Arrays of Strings.
    Returns np.array(dtype=object) instead of lists.
    """
    log(f"[Data] Initiating load for dataset: {dataset_name.upper()}...")

    config_map = {
        'imdb': ('imdb_reviews', 'text', 'label', 2),
        'ag_news': ('ag_news_subset', 'description', 'label', 4),
        'sst2': ('glue/sst2', 'sentence', 'label', 2),
        'yelp': ('yelp_polarity_reviews', 'text', 'label', 2)
    }

    if dataset_name not in config_map:
        raise ValueError(f"Dataset {dataset_name} not supported.")

    tfds_name, text_key, label_key, num_classes = config_map[dataset_name]

    try:
        log(f"[Data] Downloading/Loading TFDS: {tfds_name}...")
        ds_train, ds_info = tfds.load(tfds_name, split='train', with_info=True, as_supervised=False)

        if dataset_name == 'sst2':
            ds_test = tfds.load(tfds_name, split='validation', as_supervised=False)
        else:
            ds_test = tfds.load(tfds_name, split='test', as_supervised=False)
    except Exception as e:
        log(f"[Data] Error loading {tfds_name}: {e}")
        raise e

    def extract_data(ds, limit=None):
        texts = []
        labels = []
        if limit:
            ds = ds.take(limit)
        for ex in tfds.as_numpy(ds):
            t = ex[text_key]
            if isinstance(t, bytes):
                t = t.decode('utf-8', errors='ignore')
            else:
                t = str(t)
            texts.append(t)
            labels.append(ex[label_key])
        return np.array(texts, dtype=object), np.array(labels, dtype=np.int32)

    limit_train = 25000 if max_samples is None else max_samples
    limit_test = 5000 if max_samples is None else max_samples

    log(f"[Data] Extracting... (Train Limit: {limit_train}, Test Limit: {limit_test})")
    x_train, y_train = extract_data(ds_train, limit_train)
    x_test, y_test = extract_data(ds_test, limit_test)

    log(f"[Data] Loaded {len(x_train)} train, {len(x_test)} test samples.")
    return x_train, y_train, x_test, y_test, num_classes

# ==============================================================================
# 3. ENCODER FACTORY (Advanced Architectures)
# ==============================================================================

def get_nlp_encoder(encoder_name, num_classes, x_train_sample):
    """
    Returns (Full_Model, Feature_Extractor)
    Supports: DAN, CNN, Transformer, USE, Word2Vec, BERT, RoBERTa, NeoBERT (Electra), mmBERT
    """
    log(f"[Encoder] Constructing architecture: {encoder_name}")

    # Define Input
    inputs = layers.Input(shape=(), dtype=tf.string, name='text_input')

    # --- GROUP A: CUSTOM TRAINABLE MODELS ---
    if encoder_name in ['DAN', 'CNN', 'Transformer']:
        VOCAB_SIZE = 10000
        SEQ_LEN = 100
        vectorizer = layers.TextVectorization(max_tokens=VOCAB_SIZE, output_sequence_length=SEQ_LEN)
        vectorizer.adapt(x_train_sample[:1000])

        x = vectorizer(inputs)
        x = layers.Embedding(VOCAB_SIZE, 128)(x)

        if encoder_name == 'DAN':
            x = layers.GlobalAveragePooling1D()(x)
            x = layers.Dense(256, activation='relu')(x)
        elif encoder_name == 'CNN':
            x = layers.Conv1D(128, 5, activation='relu', padding='same')(x)
            x = layers.GlobalMaxPooling1D()(x)
            x = layers.Dense(256, activation='relu')(x)
        elif encoder_name == 'Transformer':
            att = layers.MultiHeadAttention(num_heads=2, key_dim=128)(x, x)
            x = layers.Add()([x, att])
            x = layers.LayerNormalization(epsilon=1e-6)(x)
            x = layers.GlobalAveragePooling1D()(x)
            x = layers.Dense(256, activation='relu')(x)

    # --- GROUP B: PRETRAINED MODELS (TFHub) ---
    else:
        # Configuration for TFHub models
        hub_map = {
            'USE': {
                'url': "https://tfhub.dev/google/universal-sentence-encoder/4",
                'dim': 512,
                'trainable': False
            },
            'Word2Vec': {
                # Using NNLM-128 (Modern Word2Vec equivalent)
                'url': "https://tfhub.dev/google/nnlm-en-dim128/2",
                'dim': 128,
                'trainable': False
            },
            'BERT': {
                # Official BERT Base
                'pre': "https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3",
                'enc': "https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4",
                'dim': 768,
                'trainable': False # Keep frozen for benchmarking speed
            },
            'RoBERTa': {
                # Jeongukjae implementation (TFHub standard for RoBERTa)
                'pre': "https://tfhub.dev/jeongukjae/roberta_en_cased_preprocess/1",
                'enc': "https://tfhub.dev/jeongukjae/roberta_en_cased_L-12_H-768_A-12/1",
                'dim': 768,
                'trainable': False
            },
            'NeoBERT': {
                # Mapped to ELECTRA (A newer, more efficient architecture)
                # Electra Small is efficient and powerful ("Neo" style)
                'pre': "https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3", # Electra uses BERT tokenization
                'enc': "https://tfhub.dev/google/electra_small/2",
                'dim': 256,
                'trainable': False
            },
            'mmBERT': {
                # Multi-lingual BERT
                'pre': "https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3",
                'enc': "https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/4",
                'dim': 768,
                'trainable': False
            }
        }

        if encoder_name not in hub_map:
            raise ValueError(f"Encoder {encoder_name} not found in registry.")

        config = hub_map[encoder_name]

        # Robust Wrapper for Hub Layers
        try:
            # 1. Transformer-style (Preprocessor + Encoder)
            if 'pre' in config:
                log(f"[Encoder] Loading {encoder_name} (Transformer-style)...")
                preprocessor = hub.KerasLayer(config['pre'], name=f'{encoder_name}_pre')
                encoder = hub.KerasLayer(config['enc'], trainable=config['trainable'], name=f'{encoder_name}_enc')

                def run_transformer(text_in):
                    enc_in = preprocessor(text_in)
                    enc_out = encoder(enc_in)
                    return enc_out['pooled_output']

                # Wrap in Lambda to avoid KerasTensor issues
                x = layers.Lambda(run_transformer, output_shape=(config['dim'],))(inputs)

            # 2. Embedding-style (Direct)
            else:
                log(f"[Encoder] Loading {encoder_name} (Direct Embedding)...")
                embed_layer = hub.KerasLayer(config['url'], trainable=config['trainable'], name=f'{encoder_name}_embed')

                # Reshape to ensure 1D string array input
                def run_embed(text_in):
                    return embed_layer(tf.reshape(text_in, [-1]))

                x = layers.Lambda(run_embed, output_shape=(config['dim'],))(inputs)

            # Projection to common 256 dimension for SNN
            x = layers.Dense(256, activation='relu', name='snn_projection')(x)

        except Exception as e:
            raise RuntimeError(f"Failed to load {encoder_name}. Error: {e}")

    # Classification Head (for pre-training)
    outputs = layers.Dense(num_classes, activation='softmax')(x)

    model = models.Model(inputs=inputs, outputs=outputs, name=encoder_name)
    # Extractor: Get the 256-dim output (layer before softmax)
    try:
        extract_layer = model.get_layer('snn_projection').output
    except:
        extract_layer = model.layers[-2].output

    extractor = models.Model(inputs=inputs, outputs=extract_layer)

    log(f"[Encoder] Model Built. Feature Output Shape: {extractor.output_shape}")
    return model, extractor

# ==============================================================================
# 4. ENERGY SNN (1-SADP Architecture)
# ==============================================================================

def poisson_generator(x_data, t_steps):
    """Rate coding: Features -> Spike Trains"""
    x_min = x_data.min(axis=1, keepdims=True)
    x_max = x_data.max(axis=1, keepdims=True)
    denom = (x_max - x_min)
    denom[denom == 0] = 1e-8
    x_norm = (x_data - x_min) / denom

    batch_size, num_feats = x_data.shape
    probs = x_norm[:, None, :]
    rand_draw = np.random.rand(batch_size, t_steps, num_feats)
    spikes = (rand_draw < probs).astype(np.float32)
    return spikes

class EnergySNN_1SADP:
    """
    Architecture: 1-SADP (1-layer Supervised Activity-Dependent Plasticity)
    Structure: Input -> Hidden Layer (Fixed Random Weights) -> Output Layer (Trainable)
    Note: This acts as a Liquid State Machine (LSM) or Reservoir Classifier.
    """
    def __init__(self, n_in, n_hidden, n_out):
        self.n_in = n_in
        self.n_hidden = n_hidden
        self.n_out = n_out

        self.dt = 1.0
        self.tau_m = 10.0
        self.lam = np.exp(-self.dt / self.tau_m)
        self.v_thresh = 1.0
        self.lr = 0.005
        self.ENERGY_PER_SPIKE = 5e-12

        # W1: Input -> Hidden (FIXED/RANDOM - Reservoir)
        self.W1 = np.random.normal(0, 0.1/np.sqrt(n_in), (n_in, n_hidden)).astype(np.float32)
        # W2: Hidden -> Output (TRAINABLE - Readout)
        self.W2 = np.random.normal(0, 0.1/np.sqrt(n_hidden), (n_hidden, n_out)).astype(np.float32)

        log(f"[SNN] Init 1-SADP Architecture. W1(Fixed):{self.W1.shape}, W2(Trainable):{self.W2.shape}")

    def forward(self, spikes_in, count_energy=False):
        batch_size, T, _ = spikes_in.shape
        v_h = np.zeros((batch_size, self.n_hidden))
        v_out = np.zeros((batch_size, self.n_out))

        spikes_h_rec = []
        spikes_out_rec = []
        total_spikes = 0

        for t in range(T):
            inp_t = spikes_in[:, t, :]

            # Layer 1 (Fixed)
            I_h = np.dot(inp_t, self.W1)
            v_h = v_h * self.lam + I_h
            s_h = (v_h >= self.v_thresh).astype(np.float32)
            v_h = v_h * (1 - s_h)

            # Layer 2 (Trainable Readout)
            I_out = np.dot(s_h, self.W2)
            v_out = v_out * self.lam + I_out
            s_out = (v_out >= 1.0).astype(np.float32)
            v_out = v_out * (1 - s_out)

            spikes_h_rec.append(s_h)
            spikes_out_rec.append(s_out)

            if count_energy:
                total_spikes += (np.sum(s_h) + np.sum(s_out))

        spikes_h_rec = np.stack(spikes_h_rec, axis=1)
        spikes_out_rec = np.stack(spikes_out_rec, axis=1)

        energy_j = total_spikes * self.ENERGY_PER_SPIKE if count_energy else 0
        return spikes_h_rec, spikes_out_rec, energy_j

    def train_step(self, x_batch, y_batch, t_steps):
        spikes_in = poisson_generator(x_batch, t_steps)
        s_h, s_out, _ = self.forward(spikes_in)

        rate_h = np.mean(s_h, axis=1)
        rate_out = np.mean(s_out, axis=1)

        # Delta Rule (Activity-Dependent Plasticity)
        y_onehot = np.eye(self.n_out)[y_batch]
        error = y_onehot - rate_out

        # Update Readout (W2)
        dW2 = np.dot(rate_h.T, error)
        self.W2 += self.lr * dW2

        preds = np.argmax(rate_out, axis=1)
        return np.mean(preds == y_batch)

# ==============================================================================
# 5. TRAINING LOOP
# ==============================================================================

def train_benchmark(dataset_name, encoder_name, snn_epochs=25):
    log("==================================================================")
    log(f"STARTING PIPELINE: Dataset={dataset_name} | Encoder={encoder_name}")
    log("==================================================================")

    # 1. Load Data
    x_train, y_train, x_test, y_test, num_classes = load_nlp_dataset(dataset_name, max_samples=10000)

    # 2. Setup Encoder
    full_model, extractor = get_nlp_encoder(encoder_name, num_classes, x_train)
    full_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    log("[Encoder] Starting ANN Pre-training (1 Epoch)...")
    x_train = np.array(x_train)
    y_train = np.array(y_train)

    t0 = time.time()
    hist = full_model.fit(x=x_train, y=y_train, epochs=1, batch_size=32, verbose=1)
    enc_time = time.time()-t0
    log(f"[Encoder] ANN Training Complete. Time: {enc_time:.2f}s | Final Acc: {hist.history['accuracy'][-1]:.4f}")

    # 3. Extract Features
    log("[Features] Generating Static Embeddings for SNN...")
    t0 = time.time()
    # Batch size kept conservative for heavy models (RoBERTa/BERT)
    x_train_feats = extractor.predict(x_train, batch_size=64, verbose=0)
    x_test_feats = extractor.predict(x_test, batch_size=64, verbose=0)
    feat_time = time.time()-t0
    log(f"[Features] Extraction Complete. Time: {feat_time:.2f}s")
    log(f"[Features] Embedding Shape: {x_train_feats.shape}")

    # 4. SNN
    log("[SNN] Info: Running 1-SADP (Single Layer Supervised Plasticity)")
    snn = EnergySNN_1SADP(n_in=x_train_feats.shape[1], n_hidden=256, n_out=num_classes)

    metrics_log = []
    batch_size = 64

    log(f"[SNN] Starting SNN Training Loop ({snn_epochs} Epochs)...")

    for ep in range(snn_epochs):
        ep_start = time.time()

        # Shuffle
        idx = np.random.permutation(len(x_train_feats))
        x_shuff, y_shuff = x_train_feats[idx], y_train[idx]

        accs = []
        for i in range(0, len(x_shuff), batch_size):
            xb = x_shuff[i:i+batch_size]
            yb = y_shuff[i:i+batch_size]
            acc = snn.train_step(xb, yb, t_steps=25)
            accs.append(acc)

        ep_time = time.time() - ep_start
        train_acc = np.mean(accs)

        # Energy Check on subset
        idx_test = np.random.choice(len(x_test_feats), min(500, len(x_test_feats)), replace=False)
        test_spikes = poisson_generator(x_test_feats[idx_test], t_steps=25)
        _, _, energy = snn.forward(test_spikes, count_energy=True)

        metrics_log.append({
            "dataset": dataset_name,
            "encoder": encoder_name,
            "epoch": ep+1,
            "train_acc": train_acc,
            "time_epoch": ep_time,
            "energy_sample_j": energy,
            "enc_time": enc_time,
            "feat_time": feat_time
        })
        log(f"  [Epoch {ep+1}/{snn_epochs}] Acc: {train_acc:.4f} | Time: {ep_time:.2f}s | Energy (Sample): {energy:.2e} J")

    log("[SNN] Training Complete.")
    return metrics_log