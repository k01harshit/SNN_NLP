# -*- coding: utf-8 -*-
"""NLP SNN Benchmarking Functions 50

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MZaD37J2udsnF7by17eF2msY8XbHr1MT
"""

import numpy as np
import tensorflow as tf
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from tqdm import trange, tqdm
import tensorflow_datasets as tfds
from tensorflow.keras import layers, models, regularizers
from sklearn.metrics import accuracy_score
import time
import os

# Set seeds for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# ==============================================================================
# 1. DATASET LOADING & PREPROCESSING
# ==============================================================================

MAX_VOCAB_SIZE = 10000
MAX_SEQ_LEN = 100  # Truncate/Pad text to this length

def load_nlp_dataset(dataset_name):
    """
    Loads NLP datasets, tokenizes, and prepares them for the pipeline.
    Returns: (x_train, y_train), (x_test, y_test), num_classes
    """
    print(f"\n[Data] Loading {dataset_name}...")

    if dataset_name == 'imdb':
        # Binary Sentiment (2 classes)
        # Using Keras built-in for simplicity in this env, but TFDS is robust
        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=MAX_VOCAB_SIZE)
        num_classes = 2

    elif dataset_name == 'ag_news':
        # News Topic (4 classes)
        # Simulating loading via TFDS -> Numpy
        ds_train, info = tfds.load('ag_news_subset', split='train', with_info=True, as_supervised=True)
        ds_test = tfds.load('ag_news_subset', split='test', as_supervised=True)
        num_classes = 4

        # Helper to convert TFDS to Numpy
        def tfds_to_numpy(ds):
            texts, labels = [], []
            for t, l in tfds.as_numpy(ds):
                texts.append(t)
                labels.append(l)
            return np.array(texts), np.array(labels)

        x_train, y_train = tfds_to_numpy(ds_train)
        x_test, y_test = tfds_to_numpy(ds_test)

        # Tokenizer for raw text datasets
        vectorizer = layers.TextVectorization(max_tokens=MAX_VOCAB_SIZE, output_sequence_length=MAX_SEQ_LEN)
        vectorizer.adapt(x_train)

        x_train = vectorizer(x_train).numpy()
        x_test = vectorizer(x_test).numpy()

    elif dataset_name == 'trec':
        # Question Classification (6 classes)
        ds_train, info = tfds.load('trec', split='train', with_info=True, as_supervised=True)
        ds_test = tfds.load('trec', split='test', as_supervised=True)
        num_classes = 6

        def tfds_to_numpy(ds):
            texts, labels = [], []
            for t, l in tfds.as_numpy(ds):
                texts.append(t)
                labels.append(l)
            return np.array(texts), np.array(labels)

        x_train, y_train = tfds_to_numpy(ds_train)
        x_test, y_test = tfds_to_numpy(ds_test)

        vectorizer = layers.TextVectorization(max_tokens=MAX_VOCAB_SIZE, output_sequence_length=MAX_SEQ_LEN)
        vectorizer.adapt(x_train)
        x_train = vectorizer(x_train).numpy()
        x_test = vectorizer(x_test).numpy()

    else:
        raise ValueError("Dataset not supported. Choose 'imdb', 'ag_news', or 'trec'.")

    # Ensure padding for IMDB (which comes as lists of ints)
    if dataset_name == 'imdb':
        x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=MAX_SEQ_LEN)
        x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=MAX_SEQ_LEN)

    print(f"[Data] {dataset_name} ready. Train shape: {x_train.shape}, Classes: {num_classes}")
    return (x_train, y_train), (x_test, y_test), num_classes

# ==============================================================================
# 2. ENCODER ARCHITECTURES (The "Pretrained" Candidates)
# ==============================================================================

class AttentionLayer(layers.Layer):
    """Custom Attention Layer for BiLSTM+Attn Model."""
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(name="att_weight", shape=(input_shape[-1], 1), initializer="normal")
        self.b = self.add_weight(name="att_bias", shape=(input_shape[1], 1), initializer="zeros")
        super(AttentionLayer, self).build(input_shape)

    def call(self, x):
        # x shape: (batch, seq_len, hidden_dim)
        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)
        a = tf.keras.backend.softmax(e, axis=1) # Attention weights
        output = x * a
        return tf.keras.backend.sum(output, axis=1) # Context vector

def get_nlp_encoder(encoder_name, num_classes, embedding_dim=128):
    """
    Returns a Keras model for the specified NLP architecture.
    """
    inputs = layers.Input(shape=(MAX_SEQ_LEN,))

    # Common Embedding Layer
    x = layers.Embedding(input_dim=MAX_VOCAB_SIZE, output_dim=embedding_dim, input_length=MAX_SEQ_LEN)(inputs)

    # 1. DAN (Deep Averaging Network) - Simple
    if encoder_name == 'DAN':
        x = layers.GlobalAveragePooling1D()(x)
        x = layers.Dense(256, activation='relu')(x)

    # 2. CNN-1D - Structural (N-grams)
    elif encoder_name == 'CNN':
        x = layers.Conv1D(128, 5, activation='relu')(x)
        x = layers.GlobalMaxPooling1D()(x)
        x = layers.Dense(256, activation='relu')(x)

    # 3. LSTM - Sequential
    elif encoder_name == 'LSTM':
        x = layers.LSTM(128)(x) # Returns last state
        x = layers.Dense(256, activation='relu')(x)

    # 4. BiLSTM + Attention - Advanced Sequence
    elif encoder_name == 'BiLSTM_Attn':
        x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)
        x = AttentionLayer()(x)
        x = layers.Dense(256, activation='relu')(x)

    # 5. Transformer Encoder - State of the Art (Simplified Block)
    elif encoder_name == 'Transformer':
        # Multi-Head Attention
        attention_output = layers.MultiHeadAttention(num_heads=4, key_dim=embedding_dim)(x, x)
        x = layers.Add()([x, attention_output]) # Residual
        x = layers.LayerNormalization(epsilon=1e-6)(x)

        # Feed Forward
        ffn_output = layers.Dense(embedding_dim, activation="relu")(x)
        ffn_output = layers.Dense(embedding_dim)(ffn_output)
        x = layers.Add()([x, ffn_output]) # Residual
        x = layers.LayerNormalization(epsilon=1e-6)(x)

        x = layers.GlobalAveragePooling1D()(x)
        x = layers.Dense(256, activation='relu')(x)

    else:
        raise ValueError(f"Unknown encoder: {encoder_name}")

    # Output logic:
    # 1. Prediction head (for pre-training)
    outputs = layers.Dense(num_classes, activation='softmax')(x)

    model = models.Model(inputs=inputs, outputs=outputs, name=encoder_name)

    # 2. Feature Extractor (intermediate layer for SNN input)
    # We want the 256-dim vector before the softmax
    extractor = models.Model(inputs=inputs, outputs=model.layers[-2].output)

    return model, extractor

# ==============================================================================
# 3. SNN IMPLEMENTATION (NumPy)
# ==============================================================================

def poisson_generator(x_data, t_steps):
    """
    Converts dense feature vectors to Poisson spike trains.
    x_data: (batch_size, num_features) normalized [0,1]
    returns: (batch_size, t_steps, num_features)
    """
    # Normalize features to [0,1] for rate coding
    x_norm = (x_data - x_data.min()) / (x_data.max() - x_data.min() + 1e-8)

    batch_size, num_feats = x_data.shape
    spikes = np.random.rand(batch_size, t_steps, num_feats) < x_norm[:, None, :]
    return spikes.astype(np.float32)

class SNN_Model:
    def __init__(self, n_in, n_hidden, n_out, architecture='1SADP'):
        self.n_in = n_in
        self.n_hidden = n_hidden
        self.n_out = n_out
        self.arch = architecture

        # Hyperparameters
        self.dt = 1.0
        self.tau_m = 10.0
        self.lam = np.exp(-self.dt / self.tau_m) # Leak
        self.v_thresh_base = 1.0
        self.lr_base = 0.005

        # Initialization
        # Input -> Hidden
        self.W1 = np.random.normal(0, 0.1, (n_in, n_hidden)).astype(np.float32)
        self.theta_h = np.ones(n_hidden, dtype=np.float32) * self.v_thresh_base

        # Hidden -> Output (or Hidden1 -> Hidden2)
        self.W2 = np.random.normal(0, 0.1, (n_hidden, n_out)).astype(np.float32)

        if self.arch == '2SADP':
            self.W1_2 = np.random.normal(0, 0.1, (n_hidden, n_hidden)).astype(np.float32)
            self.theta_h2 = np.ones(n_hidden, dtype=np.float32) * self.v_thresh_base
            # Re-map W2 to be Hidden2 -> Output
            self.W2 = np.random.normal(0, 0.1, (n_hidden, n_out)).astype(np.float32)
        else:
            self.W1_2 = None

        # Traces for SADP
        self.trace_decay = 0.95

    def forward(self, spikes_in):
        """
        Runs the simulation for T steps.
        spikes_in: (batch, T, n_in)
        """
        batch_size, T, _ = spikes_in.shape

        # Potentials
        v_h = np.zeros((batch_size, self.n_hidden))
        v_out = np.zeros((batch_size, self.n_out))

        # Spike recordings
        spikes_h_rec = []
        spikes_out_rec = []

        if self.arch == '2SADP':
            v_h2 = np.zeros((batch_size, self.n_hidden))
            spikes_h2_rec = []

        for t in range(T):
            inp_t = spikes_in[:, t, :]

            # --- Layer 1: Input -> Hidden ---
            # I = W * s
            I_h = np.dot(inp_t, self.W1)
            v_h = v_h * self.lam + I_h

            s_h = (v_h >= self.theta_h).astype(np.float32)
            v_h = v_h * (1 - s_h) # Reset
            spikes_h_rec.append(s_h)

            # --- Layer 2/Output logic ---
            if self.arch == '1SADP':
                I_out = np.dot(s_h, self.W2)
                v_out = v_out * self.lam + I_out
                # No threshold for output in this readout scheme (often rate-based),
                # or use large threshold. Here we accumulate potential for classification.
                # But to keep dynamics, let's spike.
                s_out = (v_out >= 1.0).astype(np.float32)
                v_out = v_out * (1 - s_out)
                spikes_out_rec.append(s_out)

            elif self.arch == '2SADP':
                I_h2 = np.dot(s_h, self.W1_2)
                v_h2 = v_h2 * self.lam + I_h2
                s_h2 = (v_h2 >= self.theta_h2).astype(np.float32)
                v_h2 = v_h2 * (1 - s_h2)
                spikes_h2_rec.append(s_h2)

                I_out = np.dot(s_h2, self.W2)
                v_out = v_out * self.lam + I_out
                s_out = (v_out >= 1.0).astype(np.float32)
                v_out = v_out * (1 - s_out)
                spikes_out_rec.append(s_out)

        # Stack results
        spikes_h_rec = np.stack(spikes_h_rec, axis=1) # (B, T, H)
        spikes_out_rec = np.stack(spikes_out_rec, axis=1)

        if self.arch == '2SADP':
            spikes_h2_rec = np.stack(spikes_h2_rec, axis=1)
            return spikes_h_rec, spikes_h2_rec, spikes_out_rec, v_out

        return spikes_h_rec, None, spikes_out_rec, v_out

    def train_step(self, x_batch, y_batch, t_steps):
        """
        Simplified supervised learning for SNN (Feedback alignment / Hebbian-like).
        """
        batch_size = x_batch.shape[0]

        # 1. Convert to spikes
        spikes_in = poisson_generator(x_batch, t_steps)

        # 2. Forward
        s_h, s_h2, s_out, v_final = self.forward(spikes_in)

        # 3. Compute Rate (Mean firing)
        rate_h = np.mean(s_h, axis=1)
        rate_out = np.mean(s_out, axis=1)
        if self.arch == '2SADP':
            rate_h2 = np.mean(s_h2, axis=1)

        # 4. Target construction (One-hot)
        y_onehot = np.zeros((batch_size, self.n_out))
        y_onehot[np.arange(batch_size), y_batch] = 1.0

        # 5. Weight Updates (Hebbian / Delta)
        # Output Error: (Target - Actual)
        # Using a simple delta rule on the rates
        error = (y_onehot - rate_out)

        # Update W2 (Hidden -> Output)
        # dW = lr * Pre_Rate.T * Error
        if self.arch == '1SADP':
            dW2 = np.dot(rate_h.T, error)
            self.W2 += self.lr_base * dW2
        else:
            dW2 = np.dot(rate_h2.T, error)
            self.W2 += self.lr_base * dW2

            # Backprop-like error for W1_2 (Feedback Alignment)
            # Propagate error back through random feedback or transpose
            error_h2 = np.dot(error, self.W2.T) * (rate_h2 > 0) # ReLU derivative approx
            dW1_2 = np.dot(rate_h.T, error_h2)
            self.W1_2 += self.lr_base * dW1_2

        # Optional: Unsupervised STDP/SADP for W1 could go here
        # For this benchmark, we focus on the classification head training
        # assuming the Encoder did the heavy lifting for feature extraction.

        # Return accuracy for this batch
        preds = np.argmax(rate_out, axis=1)
        acc = np.mean(preds == y_batch)
        return acc

# ==============================================================================
# 4. TRAINING & EVALUATION LOOP
# ==============================================================================

def train_snn_pipeline(dataset_name, encoder_name, architecture, t_steps,
                       epochs=5, encoder_epochs=5):
    """
    Full Pipeline:
    1. Load Data
    2. Train/Load Encoder (Simulating Pre-training)
    3. Extract Features
    4. Train SNN
    """
    start_total = time.time()

    # --- 1. Load Data ---
    (x_train_raw, y_train), (x_test_raw, y_test), num_classes = load_nlp_dataset(dataset_name)

    # --- 2. Encoder Phase ---
    print(f"\n[Encoder] Initializing {encoder_name}...")
    full_model, extractor = get_nlp_encoder(encoder_name, num_classes)

    full_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    print(f"[Encoder] Pre-training {encoder_name} on {dataset_name} for {encoder_epochs} epochs...")
    t0 = time.time()
    full_model.fit(x_train_raw, y_train, epochs=encoder_epochs, batch_size=64, validation_split=0.1, verbose=1)
    print(f"[Encoder] Training done in {time.time() - t0:.2f}s")

    # --- 3. Feature Extraction ---
    print("[Encoder] Extracting features for SNN...")
    # These are dense vectors (N, 256)
    x_train_feats = extractor.predict(x_train_raw, verbose=0)
    x_test_feats = extractor.predict(x_test_raw, verbose=0)

    feat_dim = x_train_feats.shape[1]
    print(f"[Features] Shape: {x_train_feats.shape}")

    # --- 4. SNN Phase ---
    print(f"\n[SNN] Initializing {architecture} (T={t_steps})...")
    snn = SNN_Model(n_in=feat_dim, n_hidden=300, n_out=num_classes, architecture=architecture)

    batch_size = 64
    n_batches = len(x_train_feats) // batch_size

    acc_history = []

    print(f"[SNN] Training for {epochs} epochs...")
    snn_start = time.time()
    for ep in range(epochs):
        # Shuffle
        indices = np.arange(len(x_train_feats))
        np.random.shuffle(indices)
        x_shuff = x_train_feats[indices]
        y_shuff = y_train[indices]

        ep_acc = []
        pbar = trange(n_batches, desc=f"Epoch {ep+1}/{epochs}", leave=False)
        for b in pbar:
            xb = x_shuff[b*batch_size : (b+1)*batch_size]
            yb = y_shuff[b*batch_size : (b+1)*batch_size]

            acc = snn.train_step(xb, yb, t_steps)
            ep_acc.append(acc)
            pbar.set_postfix(acc=f"{np.mean(ep_acc):.4f}")

        acc_history.append(np.mean(ep_acc))
        print(f"  Epoch {ep+1}: Train Acc = {np.mean(ep_acc):.4f}")

    snn_time = time.time() - snn_start

    # --- 5. Final Evaluation ---
    print("[SNN] Evaluating on Test Set...")
    # Process test set in batches to save memory
    test_preds = []
    n_test_batches = len(x_test_feats) // batch_size

    for b in range(n_test_batches + 1):
        xb = x_test_feats[b*batch_size : (b+1)*batch_size]
        if len(xb) == 0: continue

        spikes = poisson_generator(xb, t_steps)
        _, _, s_out, _ = snn.forward(spikes)

        # Rate decoding
        rates = np.mean(s_out, axis=1)
        preds = np.argmax(rates, axis=1)
        test_preds.extend(preds)

    # Truncate y_test to match batch drops if any (simplification)
    y_test_trunc = y_test[:len(test_preds)]
    final_acc = accuracy_score(y_test_trunc, test_preds)

    print(f"RESULTS: {dataset_name} | {encoder_name} | {architecture} -> Test Acc: {final_acc:.4f}")

    return {
        "dataset": dataset_name,
        "encoder": encoder_name,
        "architecture": architecture,
        "timesteps": t_steps,
        "test_acc": final_acc,
        "snn_train_time": snn_time
    }